#+latex_header: \usepackage[utf8]{inputenc}

* Trigger Scale Factors for the /H H \rightarrow b b \tau \tau/ Resonant Analysis

This framework calculates and stores "intersection efficiencies" for the application of the "inclusion method" [1], to be used for the /H H \rightarrow b b \tau \tau/ resonant analysis by the CMS experiment. More details available in [[https://indico.cern.ch/event/1143576/#13-trigger-sf-update][this presentation]].

The processing starts from skimmed ([[https://github.com/LLRCMS/KLUBAnalysis][KLUB]]) Ntuples. The framework is managed by [[https://github.com/spotify/luigi][luigi]] (see ~run_workflow.py~), which runs local tasks and creates a Direct Acyclic Graph (DAG) which runs on [[https://htcondor.readthedocs.io/en/latest/index.html][HTCondor]].

** Requirements

- ~python3~ (tested on ~3.9.6~)
- ~luigi~ (available in ~CMSSW~ after running ~cmsenv~ and using ~python3~).

** Tasks

1. Binning (manual or equal width with upper 5% quantile removal)
   - is the only job which runs locally (/i.e./, not on HTCondor)
2. *Histos_SKIM_**: filling efficiencies numerator and denominator histograms
3. *HaddHisto**: add histograms together per data type (Data and all MCs)
4. *HaddHistoAgg**: add histograms for Data and MC
5. *EfficienciesAndSF**: calculate efficiencies by dividing the histograms obtained in point #2
6. *EffAndAgg*: aggregate efficiencies and scale factors in one file per channel
7. *Discriminator_**: choose the variables to be used to calculate the final union efficiency
8. *UnionWeightsCalculator_SKIM_**: calculate the union efficiencies (following the =inclusion method= [1])
9. *Closure*: perform a simple closure (complete closure is done outside this framework in the HH \rightarrow bb\tau \tau ~C++~ analysis code)

*** Visualize DAG

Run ~dot -Tpng dag.dot -o dag.png~ as explained [[https://research.cs.wisc.edu/htcondor/manual/v7.8/2_10DAGMan_Applications.html#SECTION0031010000000000000000][here]] (a ~dot~ file was previously created by the DAG with ~DOT dag.dot~ [[https://github.com/b-fontana/METTriggerStudies/blob/main/scripts/writeHTCondorDAGFiles.py#L73][here]]).

[[./dag.png]]

** Luigi/HTCondor Workflow

Run the submission workflow (check the meaning of the arguments by adding ~--help~):

#+NAME: running_command
#+BEGIN_SRC shell
python3 run_workflow.py --tag FullWorkflowTest --data MET --mc_process TT
#+END_SRC

If everything runs as expected, the above should run locally all local tasks (currently ~DefineBinning~ only) and launch a HTCondor DAG which encodes de dependencies of the remaining tasks and runs them in the server.

The HTCondor files are written using the =scripts/writeHTCondorDAGFiles.py= and =scripts/jobWriter.sh= files.

| Output files              | Destination folder                                           |
|---------------------------+--------------------------------------------------------------|
| ~ROOT~                      | ~/data_CMS/cms/<llr username>/TriggerScaleFactors/<some tag>/~ |
| Submission                | ~$HOME/jobs/<some tag>/<process>/submission/~                  |
| Condor (output and error) | ~$HOME/jobs/<some tag>/<process>/outputs/~                     |

You can also run each ~luigi~ task separately by running its corresponding ~python~ scripts (all support ~--help~). Inspect HTCondor's output shell and condor files for the full commands. For instance:

#+NAME: single_task
#+BEGIN_SRC shell 
python3 scripts/getTriggerEffSig.py --indir /data_CMS/cms/portales/HHresonant_SKIMS/SKIMS_2018_UL_backgrounds_test11Jan22/ --outdir /data_CMS/cms/alves/TriggerScaleFactors/UL_v1 --sample SKIMfix_TT_fullyHad --isData 0 --file output_2.root --subtag _default --channels all etau mutau tautau mumu --triggers METNoMu120 IsoTau50 --variables mht_et mhtnomu_et met_et dau2_eta dau2_pt HH_mass metnomu_et dau1_eta dau1_pt HT20 --tprefix hist_ --binedges_fname /data_CMS/cms/alves/TriggerScaleFactors/UL_v1/binedges.hdf
#+END_SRC

Variables can be configured in ~luigi_conf/__init__.py~.

** Cleanup

In order to avoid cluttering the local area with output files, a =bash= script was written to effortlessly delete them:

#+NAME: clean
#+BEGIN_SRC shell
bash triggerClean.sh --tag <any tag>
#+END_SRC

with options:

- ~-d~: debug mode, where all commands are printed to the screen and nothing is run
- ~-f~: full delete, including data produced by the HTCondor jobs (this flag is required to avoid data deletions by mistake)
- ~--tag~: tag used when producing the files (remove this options to print a message displaying all tags used in the past which were not yet removed)

-------------------------------------

** Notes on ~luigi~

The advantages of using a workflow management system as ~luigi~ are the following:

- the whole chain can be run at once
- the workflow is clearer from a code point of view
- the configuration is accessible from within a single file (~luigi.cfg~)
- when two tasks do not share dependencies they can run in parallel

*** Debugging

By passing ~--debug_workflow~, the user can obtain more information regarding the specific order tasks and their functions are run.

*** Visualizing the workflow

When using ```--scheduler central```, one can visualize the ```luigi``` workflow by accessing the correct port in the browser, specified with ```luigid --port <port_number> &```. If using ```ssh```, the port will have to be forwarded
